import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import qiskit
from qiskit import QuantumCircuit, execute, IBMQ
from qiskit.result import Counts
import qutip as qt
import pickle

# 1. Data Loading and Preprocessing
class QuantumDataset(Dataset):
    def __init__(self, real_data_path, simulated_data_path):
        # Load IBMQ real data
        self.real_data = self.load_ibmq_data(real_data_path)
        
        # Load QDataset simulated data
        with open(simulated_data_path, 'rb') as f:
            self.simulated_data = pickle.load(f)
        
        # Normalize data to [-1, 1]
        self.min_val = min(np.min(self.real_data), np.min(self.simulated_data))
        self.max_val = max(np.max(self.real_data), np.max(self.simulated_data))
        
    def load_ibmq_data(self, path):
        # Load or generate IBMQ data
        provider = IBMQ.load_account()
        backend = provider.get_backend('ibmq_lima')
        
        # Example circuit
        qc = QuantumCircuit(2)
        qc.h(0)
        qc.cx(0, 1)
        qc.measure_all()
        
        job = execute(qc, backend, shots=1000)
        counts = job.result().get_counts()
        
        # Convert counts to probability vector
        prob_vec = np.zeros(4)
        for key, val in counts.items():
            prob_vec[int(key, 2)] = val/1000
            
        return prob_vec

    def __getitem__(self, index):
        real = (self.real_data - self.min_val) / (self.max_val - self.min_val) * 2 - 1
        simulated = (self.simulated_data - self.min_val) / (self.max_val - self.min_val) * 2 - 1
        return {
            'real': torch.tensor(real, dtype=torch.float32),
            'simulated': torch.tensor(simulated, dtype=torch.float32)
        }

# 2. CycleGAN Model Architecture
class ResidualBlock(nn.Module):
    def __init__(self, in_features):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv1d(in_features, in_features, 3, padding=1),
            nn.InstanceNorm1d(in_features),
            nn.ReLU(inplace=True),
            nn.Conv1d(in_features, in_features, 3, padding=1),
            nn.InstanceNorm1d(in_features)
        )

    def forward(self, x):
        return x + self.block(x)

class Generator(nn.Module):
    def __init__(self, input_nc=4, output_nc=4, n_residual=6):
        super().__init__()
        # Initial convolution block
        model = [
            nn.Conv1d(input_nc, 64, 7, padding=3),
            nn.InstanceNorm1d(64),
            nn.ReLU(inplace=True)
        ]

        # Downsampling
        in_features = 64
        out_features = in_features*2
        model += [
            nn.Conv1d(in_features, out_features, 3, stride=2, padding=1),
            nn.InstanceNorm1d(out_features),
            nn.ReLU(inplace=True)
        ]
        
        # Residual blocks
        for _ in range(n_residual):
            model += [ResidualBlock(out_features)]

        # Upsampling
        model += [
            nn.ConvTranspose1d(out_features, in_features, 3, stride=2, padding=1, output_padding=1),
            nn.InstanceNorm1d(in_features),
            nn.ReLU(inplace=True)
        ]

        # Output layer
        model += [nn.Conv1d(64, output_nc, 7, padding=3), nn.Tanh()]

        self.model = nn.Sequential(*model)

    def forward(self, x):
        return self.model(x)

# 3. Training Setup
class CycleGANTrainer:
    def __init__(self):
        self.G = Generator()  # Simulated -> Real
        self.F = Generator()  # Real -> Simulated
        self.D_X = nn.Conv1d(4, 1, 3, padding=1)  # Discriminator for real
        self.D_Y = nn.Conv1d(4, 1, 3, padding=1)  # Discriminator for simulated

        self.optimizer_G = torch.optim.Adam(
            list(self.G.parameters()) + list(self.F.parameters()),
            lr=0.0002, betas=(0.5, 0.999)
        )
        
        self.optimizer_D = torch.optim.Adam(
            list(self.D_X.parameters()) + list(self.D_Y.parameters()),
            lr=0.0002, betas=(0.5, 0.999)
        )

        self.criterion_cycle = torch.nn.L1Loss()
        self.criterion_gan = torch.nn.MSELoss()

    def train(self, dataloader, epochs=100):
        for epoch in range(epochs):
            for batch in dataloader:
                real = batch['real'].unsqueeze(1)
                simulated = batch['simulated'].unsqueeze(1)
                
                # Train generators
                self.optimizer_G.zero_grad()
                
                # Adversarial losses
                fake_real = self.G(simulated)
                loss_G = self.criterion_gan(self.D_Y(fake_real), torch.ones_like(self.D_Y(fake_real)))
                
                fake_sim = self.F(real)
                loss_F = self.criterion_gan(self.D_X(fake_sim), torch.ones_like(self.D_X(fake_sim)))
                
                # Cycle consistency
                cycle_sim = self.F(fake_real)
                loss_cycle_sim = self.criterion_cycle(cycle_sim, simulated)
                
                cycle_real = self.G(fake_sim)
                loss_cycle_real = self.criterion_cycle(cycle_real, real)
                
                # Total loss
                total_loss = (loss_G + loss_F) + 10 * (loss_cycle_sim + loss_cycle_real)
                total_loss.backward()
                self.optimizer_G.step()

                # Train discriminators
                self.optimizer_D.zero_grad()
                
                # Real data
                loss_real = self.criterion_gan(self.D_Y(real), torch.ones_like(self.D_Y(real)))
                loss_sim = self.criterion_gan(self.D_X(simulated), torch.ones_like(self.D_X(simulated)))
                
                # Fake data
                loss_fake_real = self.criterion_gan(self.D_Y(fake_real.detach()), torch.zeros_like(self.D_Y(fake_real)))
                loss_fake_sim = self.criterion_gan(self.D_X(fake_sim.detach()), torch.zeros_like(self.D_X(fake_sim)))
                
                total_loss_D = (loss_real + loss_sim + loss_fake_real + loss_fake_sim) / 4
                total_loss_D.backward()
                self.optimizer_D.step()

if __name__ == "__main__":
    dataset = QuantumDataset("ibmq_data.pkl", "qdataset_simulated.pkl")
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    trainer = CycleGANTrainer()
    trainer.train(dataloader, epochs=100)
    
    # Save models
    torch.save(trainer.G.state_dict(), "sim2real_generator.pth")
    torch.save(trainer.F.state_dict(), "real2sim_generator.pth")
